---
# K3s High Availability Cluster Setup
# Configures 3-node HA cluster with embedded etcd on Pi CM5 nodes
# Includes Pi-specific optimizations for ARM64 and eMMC storage

- name: Install additional kernel modules for K3s (if needed)
  hosts: k3s_cluster
  become: true
  gather_facts: true

  tasks:
    - name: Check if running on Raspberry Pi
      ansible.builtin.set_fact:
        is_raspberry_pi: "{{ ansible_facts['board'] is defined and 'Raspberry Pi' in ansible_facts['board'] }}"

    - name: Install additional kernel modules for Ubuntu on Pi
      ansible.builtin.package:
        name: linux-modules-extra-raspi
        state: present
      when:
        - is_raspberry_pi
        - ansible_facts['distribution'] == 'Ubuntu'
        - ansible_facts['distribution_major_version'] | int >= 21
      tags:
        - kernel-modules

- name: Deploy K3s HA cluster
  hosts: k3s_cluster
  become: true
  gather_facts: true

  roles:
    - role: xanmanning.k3s

  post_tasks:
    - name: Create .kube directory for root
      ansible.builtin.file:
        path: /root/.kube
        state: directory
        mode: '0755'
      when: inventory_hostname in groups['control_plane']
      tags:
        - kubeconfig

    - name: Copy kubeconfig for root access
      ansible.builtin.copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: /root/.kube/config
        remote_src: true
        mode: '0600'
      when:
        - not ansible_check_mode
        - inventory_hostname in groups['control_plane']
      tags:
        - kubeconfig

    - name: Wait for K3s to be ready
      ansible.builtin.wait_for:
        port: 6443
        host: "{{ ansible_default_ipv4.address }}"
        timeout: 300
      when:
        - not ansible_check_mode
        - inventory_hostname in groups['control_plane']
      tags:
        - verification

    - name: Verify cluster nodes
      ansible.builtin.command:
        cmd: kubectl get nodes -o json --kubeconfig /etc/rancher/k3s/k3s.yaml
      register: cluster_nodes_raw
      when:
        - inventory_hostname == groups['control_plane'][0]
        - not ansible_check_mode
      tags:
        - verification
      changed_when: false
      failed_when: false
      retries: 3
      delay: 10

    - name: Parse cluster nodes information
      ansible.builtin.set_fact:
        k3s_cluster_status: "{{ cluster_nodes_raw.stdout | from_json }}"
      when:
        - inventory_hostname == groups['control_plane'][0]
        - not ansible_check_mode
        - cluster_nodes_raw is defined
        - cluster_nodes_raw.rc == 0
        - cluster_nodes_raw.stdout is defined
        - cluster_nodes_raw.stdout | length > 0
      tags:
        - verification

    - name: Categorize cluster nodes by role
      ansible.builtin.set_fact:
        control_plane_nodes: >-
          {{ k3s_cluster_status['items'] |
             selectattr('metadata.labels.node-role.kubernetes.io/control-plane', 'defined') |
             list }}
        worker_nodes: >-
          {{ k3s_cluster_status['items'] |
             rejectattr('metadata.labels.node-role.kubernetes.io/control-plane', 'defined') |
             list }}
      when:
        - inventory_hostname == groups['control_plane'][0]
        - not ansible_check_mode
        - k3s_cluster_status is defined
        - k3s_cluster_status is mapping
        - k3s_cluster_status['items'] is defined
      tags:
        - verification

    - name: Display cluster status
      ansible.builtin.debug:
        msg: |
          K3s HA cluster deployed successfully!

          Cluster Topology ({{ k3s_cluster_status['items'] | length }} nodes total):
          ================================================================

          Control Plane Nodes ({{ control_plane_nodes | length }}/{{ groups['control_plane'] | length }} expected):
          {% for node in control_plane_nodes %}
          - {{ node.metadata.name }}: {{ node.status.conditions |
            selectattr('type', 'equalto', 'Ready') | map(attribute='status') | first }}
            └─ https://{{ node.status.addresses | selectattr('type', 'equalto', 'InternalIP') | map(attribute='address') | first }}:6443
          {% endfor %}

          Worker Nodes ({{ worker_nodes | length }}/{{ groups['workers'] | length }} expected):
          {% for node in worker_nodes %}
          - {{ node.metadata.name }}: {{ node.status.conditions |
            selectattr('type', 'equalto', 'Ready') | map(attribute='status') | first }}
            └─ Labels: {{ node.metadata.labels.get('storage', 'none') }}
          {% endfor %}

          Verification Status:
          ✓ Total nodes: {{ k3s_cluster_status['items'] | length }}
          {{ '✓' if control_plane_nodes | length == groups['control_plane'] | length else '✗' }}
            Control plane: {{ control_plane_nodes | length }}/{{ groups['control_plane'] | length }}
          {{ '✓' if worker_nodes | length == groups['workers'] | length else '✗' }}
            Workers: {{ worker_nodes | length }}/{{ groups['workers'] | length }}

          To access the cluster from your local machine:
          1. Copy /etc/rancher/k3s/k3s.yaml from any server node
          2. Update server URL to point to any control plane node
          3. kubectl get nodes
      when:
        - inventory_hostname == groups['control_plane'][0]
        - not ansible_check_mode
        - k3s_cluster_status is defined
        - k3s_cluster_status is mapping
        - k3s_cluster_status['items'] is defined
        - k3s_cluster_status['items'] is sequence
        - k3s_cluster_status['items'] | length > 0
      tags:
        - verification


    - name: Display cluster verification failure
      ansible.builtin.debug:
        msg: |
          Warning: Cluster verification failed!

          The kubectl command to verify cluster nodes failed.
          This might indicate:
          - K3s is still starting up (wait a few minutes and check manually)
          - There was an error during cluster deployment
          - Network connectivity issues

          Manual verification steps:
          1. SSH to {{ groups['control_plane'][0] }}
          2. Run: sudo kubectl get nodes --kubeconfig /etc/rancher/k3s/k3s.yaml
          3. Check K3s service: sudo systemctl status k3s
          4. Check logs: sudo journalctl -u k3s -f

          Raw kubectl output: {{ cluster_nodes_raw.stdout | default('No output') }}
          Error: {{ cluster_nodes_raw.stderr | default('No error message') }}
      when:
        - inventory_hostname == groups['control_plane'][0]
        - not ansible_check_mode
        - cluster_nodes_raw is defined
        - (cluster_nodes_raw.rc != 0 or cluster_nodes_raw.stdout is not defined or cluster_nodes_raw.stdout | length == 0)
      tags:
        - verification

    - name: Display check mode information
      ansible.builtin.debug:
        msg: |
          K3s HA Cluster Deployment Preview (Check Mode)
          ============================================

          This dry-run shows what would be deployed:

          Target Infrastructure:
          {% for host in groups['control_plane'] %}
          - {{ host }}: K3s server node with embedded etcd
          {% endfor %}
          {% for host in groups['workers'] %}
          - {{ host }}: K3s agent (worker) node
          {% endfor %}

          Configuration Summary:
          - K3s Version: {{ k3s_release_version }}
          - Network: Flannel VXLAN backend
          - Cluster CIDR: {{ k3s_server['cluster-cidr'] }}
          - Service CIDR: {{ k3s_server['service-cidr'] }}
          - Disabled Components: {{ k3s_server.disable | join(', ') }}

          Control Plane Endpoints (after deployment):
          {% for host in groups['control_plane'] %}
          - https://{{ hostvars[host]['ansible_default_ipv4']['address'] }}:6443
          {% endfor %}

          To actually deploy the cluster, run: make k3s-cluster
      when:
        - inventory_hostname == groups['control_plane'][0]
        - ansible_check_mode
      tags:
        - verification
