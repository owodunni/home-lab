---
# Configuration for Beelink ME Mini N150 storage server
# Hardware: Intel N150, 6x M.2 slots, Dual 2.5G LAN
# Role: NFS storage server with MergerFS + SnapRAID for K3s cluster

# Hardware Overview
# ================
# Model: Beelink ME Mini N150
# CPU: Intel N150 (4-core, low power)
# RAM: 12-16GB LPDDR5-4800 (soldered)
# Storage: 6x M.2 slots (2230/2242/2280) - up to 24TB capacity
# Network: Dual 2.5G Ethernet + WiFi 6
# Use case: Longhorn storage node for K3s cluster distributed storage

# Longhorn Storage Configuration
# ==============================
# Uses persistent disk identifiers (WWN) to avoid device name changes
# Defines the NVMe drives to be prepared for Longhorn storage
# IMPORTANT: Fill in actual WWN identifiers after running:
#   ssh beelink "ls -l /dev/disk/by-id/ | grep nvme"

longhorn_storage_drives:
  - device: /dev/disk/by-id/nvme-CT2000P310SSD8_24454C177944
    label: LONGHORN1
  - device: /dev/disk/by-id/nvme-CT2000P310SSD8_24454C37CB1B
    label: LONGHORN2
  - device: /dev/disk/by-id/nvme-CT2000P310SSD8_24454C40D38E
    label: LONGHORN3

# LUKS Encryption Configuration
# =============================
# Full disk encryption for data-at-rest security
# Key file managed via ansible-vault for automatic unlock on boot

# LUKS key file location (encrypted via ansible-vault)
# The file group_vars/beelink_nas/luks.key is encrypted with ansible-vault
# Ansible automatically decrypts it when copying to the server
# inventory_dir points to the repository root where group_vars lives
luks_key_file: "{{ inventory_dir }}/group_vars/beelink_nas/luks.key"

# LVM Configuration
# =================
# Aggregate all encrypted drives into single volume group
# Allows Longhorn to use all storage as one pool and enables future expansion

# LVM volume group name
lvm_volume_group: longhorn-vg

# LVM logical volume name
lvm_logical_volume: longhorn-lv

# Use 100% of available space in volume group
lvm_volume_size: 100%FREE

# Filesystem Configuration
# ========================
# ext4 chosen over XFS for Longhorn compatibility and stability
# - Longhorn officially recommends ext4
# - Better stability under network issues (less corruption)
# - Required for RWX volumes (longhorn-share-manager)
# - Better small file performance (typical k8s workloads)

longhorn_filesystem: ext4

# Longhorn Configuration (MergerFS Architecture)
# ================================================
# Longhorn uses MergerFS pool at /mnt/storage/longhorn
# Benefits:
# - Part of MergerFS filesystem (SnapRAID parity protection)
# - Easy SSH access if needed
# - Separate backup path from media (no overlap with restic)
#
# Backup Strategy (No Double Backups):
# - Longhorn: /mnt/storage/longhorn → s3://longhorn-backups (automatic daily)
# - restic:   /mnt/storage/media    → s3://restic-backups (automatic daily)
# - SnapRAID: Parity protection for both paths

# Longhorn data path on Beelink (subdirectory of MergerFS pool)
longhorn_data_path: /mnt/storage/longhorn

# Storage reservation (leave some space for filesystem overhead)
# Longhorn will use 90% of available space, leaving 10% as buffer
longhorn_storage_reserved_percentage: 10

# ============================================================================
# MergerFS + SnapRAID Configuration (New Architecture)
# ============================================================================
# This configuration replaces the Longhorn LVM setup with filesystem-based
# storage using MergerFS for pooling and SnapRAID for parity protection.
#
# Benefits:
# - Direct SSH access to files (no need to exec into pods)
# - Better backup reliability for large volumes (restic to MinIO S3)
# - Easy expansion (add drives one at a time)
# - Survive single disk failure (SnapRAID parity recovery)
# - More efficient (4TB usable vs 2TB with Longhorn 3x replication)

# Storage Drives Configuration
# ============================
# Define individual drives for data and parity
# IMPORTANT: Reuses existing LUKS-encrypted devices from Longhorn setup

storage_drives:
  - device: /dev/disk/by-id/nvme-CT2000P310SSD8_24454C177944
    label: disk1
    mount_point: /mnt/disk1
  - device: /dev/disk/by-id/nvme-CT2000P310SSD8_24454C37CB1B
    label: disk2
    mount_point: /mnt/disk2
  - device: /dev/disk/by-id/nvme-CT2000P310SSD8_24454C40D38E
    label: parity1
    mount_point: /mnt/parity1

# LUKS Configuration (Updated for Individual Mounts)
# ==================================================
# Reuses existing LUKS keys, just changes device names for clarity
#
# NOTE: The old Longhorn setup used longhorn1_crypt, longhorn2_crypt, longhorn3_crypt
# These will be recreated as disk1_crypt, disk2_crypt, parity1_crypt
# This is handled by the reconfiguration playbook

# Comment out old LUKS config (kept for reference)
# luks_crypt_devices:
#   - name: longhorn1_crypt
#     device: "{{ longhorn_storage_drives[0].device }}"
#   - name: longhorn2_crypt
#     device: "{{ longhorn_storage_drives[1].device }}"
#   - name: longhorn3_crypt
#     device: "{{ longhorn_storage_drives[2].device }}"

# New LUKS config for MergerFS + SnapRAID
luks_crypt_devices:
  - name: disk1_crypt
    device: "{{ storage_drives[0].device }}"
  - name: disk2_crypt
    device: "{{ storage_drives[1].device }}"
  - name: parity1_crypt
    device: "{{ storage_drives[2].device }}"

# Filesystem Configuration
# ========================
storage_filesystem: ext4 # Consistent with Longhorn choice
storage_mount_options: "defaults,noatime"

# MergerFS Configuration
# ======================
# Pool data drives into single mount point for unified storage

mergerfs_mount_point: /mnt/storage

# Data drives to pool (first 2 drives)
mergerfs_data_drives:
  - /mnt/disk1
  - /mnt/disk2

# MergerFS mount options
# - allow_other: Allow non-root users to access
# - use_ino: Preserve inode numbers (required for hardlinks)
# - cache.files=partial: Cache file metadata for performance
# - dropcacheonclose=true: Drop kernel cache when file closed
# - category.create=mfs: Use "most free space" policy for new files
mergerfs_options: "defaults,allow_other,use_ino,cache.files=partial,dropcacheonclose=true,category.create=mfs"

# SnapRAID Configuration
# ======================
# Parity protection for disaster recovery

# Parity file location(s)
snapraid_parity_drives:
  - /mnt/parity1/snapraid.parity

# Data drives configuration
snapraid_data_drives:
  - path: /mnt/disk1
    name: d1
  - path: /mnt/disk2
    name: d2

# Content file locations (metadata storage)
# Multiple locations for redundancy - if one disk fails, metadata survives
snapraid_content_files:
  - /var/snapraid.content
  - /mnt/disk1/.snapraid.content
  - /mnt/disk2/.snapraid.content

# NFS Server Configuration
# ========================
# Export storage to K3s pods for media stack access

nfs_exports:
  - path: /mnt/storage
    clients: "10.42.0.0/16(rw,sync,no_subtree_check,no_root_squash,fsid=0)" # K3s pod network
